{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ecco\n",
    "from transformers import AutoTokenizer\n",
    "import japanize_matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Papermillで上書きされるパラメータ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "text = \"吾輩は猫である。名前は\"\n",
    "model_name = \"rinna/japanese-gpt2-xsmall\"\n",
    "max_length = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eccoでの処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = {\n",
    "    'embedding': \"transformer.wte.weight\",\n",
    "    'type': 'causal',\n",
    "    'activations': ['mlp\\.c_proj'],\n",
    "    \"token_prefix\": \"_\",\n",
    "    \"partial_token_prefix\": \"\",\n",
    "}\n",
    "lm = ecco.from_pretrained(model_name, model_config=model_config)\n",
    "output = lm.generate(text, generate=max_length, do_sample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.layer_predictions(position=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.layer_predictions(position=10, layer=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.rankings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "output.rankings_watch(\n",
    "    [\n",
    "        tokenizer.vocab[\"犬\"],\n",
    "        tokenizer.vocab[\"猫\"],\n",
    "        tokenizer.vocab[\"鳥\"],\n",
    "        tokenizer.vocab[\"太郎\"],\n",
    "        tokenizer.vocab[\"一郎\"],\n",
    "        tokenizer.vocab[\"まだ\"],\n",
    "        tokenizer.vocab[\"の\"],\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a2b06d188e54ecac4ab4fe73a8a010ef0b9e00f7cf249794c189512420a294c4"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
